% Part: incompleteness
% Chapter: introduction
% Section: historical-background

\documentclass[../../../include/open-logic-section]{subfiles}

\begin{document}

\olfileid{inc}{int}{bgr}

\olsection{Historical Background}

In this section, we will briefly discuss historical developments that
will help put the incompleteness theorems in context. In particular,
we will give a very sketchy overview of the history of mathematical
logic; and then say a few words about the history of the foundations
of mathematics.

\begin{digress}
  The phrase ``mathematical logic'' is ambiguous. One can interpret the
word ``mathematical'' as describing the subject matter, as in, ``the
logic of mathematics,'' denoting the principles of mathematical
reasoning; or as describing the methods, as in ``the mathematics of
logic,'' denoting a mathematical study of the principles of reasoning.
The account that follows involves mathematical logic in both senses,
often at the same time.
\end{digress}

The study of logic began, essentially, with Aristotle, who lived
approximately 384--322 \textsc{bce}. His \emph{Categories}, \emph{Prior
  analytics}, and \emph{Posterior analytics} include systematic
studies of the principles of scientific reasoning, including a
thorough and systematic study of the syllogism.

Aristotle's logic dominated scholastic philosophy through the middle
ages; indeed, as late as eighteenth century Kant maintained that
Aristotle's logic was perfect and in no need of revision. But the
theory of the syllogism is far too limited to model anything but
the most superficial aspects of mathematical reasoning. A century
earlier, Leibniz, a contemporary of Newton's, imagined a complete
``calculus'' for logical reasoning, and made some rudimentary steps
towards designing such a calculus, essentially describing a version of
propositional logic.

The nineteenth century was a watershed for logic. In 1854 George Boole
wrote \emph{The Laws of Thought}, with a thorough algebraic study of
propositional logic that is not far from modern presentations. In 1879
Gottlob Frege published his \emph{Begriffsschrift} (Concept writing)
which extends propositional logic with quantifiers and relations, and
thus includes first-order logic. In fact, Frege's logical systems
included higher-order logic as well, and more. In his \emph{Basic Laws
  of Arithmetic}, Frege set out to show that all of arithmetic could
be derived in his Begriffsschrift from purely logical
assumption. Unfortunately, these assumptions turned out to be
inconsistent, as Russell showed in 1902. But setting aside the
inconsistent axiom, Frege more or less invented modern logic
singlehandedly, a startling achievement. Quantificational logic was
also developed independently by algebraically-minded thinkers after
Boole, including Peirce and Schr\"oder.

Let us now turn to developments in the foundations of mathematics. Of
course, since logic plays an important role in mathematics, there is a
good deal of interaction with the developments just described. For
example, Frege developed his logic with the explicit purpose of
showing that all of mathematics could be based solely on his logical
framework; in particular, he wished to show that mathematics consists
of a priori \emph{analytic} truths instead of, as Kant had maintained,
a priori \emph{synthetic} ones.

Many take the birth of mathematics proper to have occurred with the
Greeks. Euclid's \emph{Elements}, written around 300 B.C., is already
a mature representative of Greek mathematics, with its emphasis on
rigor and precision. The definitions and proofs in Euclid's
\emph{Elements} survive more or less in tact in high school geometry
textbooks today (to the extent that geometry is still taught in high
schools). This model of mathematical reasoning has been held to be a
paradigm for rigorous argumentation not only in mathematics but in
branches of philosophy as well. (Spinoza even presented moral and
religious arguments in the Euclidean style, which is strange to see!)

Calculus was invented by Newton and Leibniz in the seventeenth
century. (A fierce priority dispute raged for centuries, but most
scholars today hold that the two developments were for the most part
independent.)  Calculus involves reasoning about, for example,
infinite sums of infinitely small quantities; these features fueled
criticism by Bishop Berkeley, who argued that belief in God was no
less rational than the mathematics of his time. The methods of
calculus were widely used in the eighteenth century, for example by
Leonhard Euler, who used calculations involving infinite sums with
dramatic results.

In the nineteenth century, mathematicians tried to address Berkeley's
criticisms by putting calculus on a firmer foundation. Efforts by
Cauchy, Weierstrass, Bolzano, and others led to our contemporary
definitions of limits, continuity, differentiation, and integration in
terms of ``epsilons and deltas,'' in other words, devoid of any
reference to infinitesimals. Later in the century, mathematicians
tried to push further, and explain all aspects of calculus, including
the real numbers themselves, in terms of the natural numbers.
(Kronecker: ``God created the whole numbers, all else is the work of
man.'') In 1872, Dedekind wrote ``Continuity and the irrational
numbers,'' where he showed how to ``construct'' the real numbers as
sets of rational numbers (which, as you know, can be viewed as pairs
of natural numbers); in 1888 he wrote ``Was sind und was sollen die
Zahlen'' (roughly, ``What are the natural numbers, and what should
they be?'') which aimed to explain the natural numbers in purely
``logical'' terms. In 1887 Kronecker wrote ``\"Uber den Zahlbegriff''
(``On the concept of number'') where he spoke of representing all
mathematical object in terms of the integers; in 1889 Giuseppe Peano
gave formal, symbolic axioms for the natural numbers.

The end of the nineteenth century also brought a new boldness in dealing
with the infinite. Before then, infinitary objects and structures
(like the set of natural numbers) were treated gingerly; ``infinitely
many'' was understood as ``as many as you want,'' and ``approaches in
the limit'' was understood as ``gets as close as you want.'' But Georg
Cantor showed that it was possible to take the infinite at face
value. Work by Cantor, Dedekind, and others help to introduce the
general set-theoretic understanding of mathematics that is now widely accepted.

This brings us to twentieth century developments in logic and foundations.
In 1902 Russell discovered the paradox in Frege's logical system. In
1904 Zermelo proved Cantor's well-ordering principle, using the
so-called ``axiom of choice''; the legitimacy of this axiom prompted a
good deal of debate. Between 1910 and 1913 the three volumes of
Russell and Whitehead's \emph{Principia Mathematica} appeared,
extending the Fregean program of establishing mathematics on logical
grounds. Unfortunately, Russell and Whitehead were forced to adopt two
principles that seemed hard to justify as purely logical: an axiom of
infinity and an axiom of ``reducibility.'' In the 1900's Poincar\'e
criticized the use of ``impredicative definitions'' in mathematics,
and in the 1910's Brouwer began proposing to refound all of
mathematics in an ``intuitionistic'' basis, which avoided the use of
the law of the excluded middle ($!A \lor \lnot !A$).

Strange days indeed!{} The program of reducing all of mathematics to
logic is now referred to as ``logicism,'' and is commonly viewed as
having failed, due to the difficulties mentioned above. The program of
developing mathematics in terms of intuitionistic mental constructions
is called ``intuitionism,'' and is viewed as posing overly severe
restrictions on everyday mathematics. Around the turn of the century,
David Hilbert, one of the most influential mathematicians of all time,
was a strong supporter of the new, abstract methods introduced by
Cantor and Dedekind: ``no one will drive us from the paradise that
Cantor has created for us.'' At the same time, he was sensitive to
foundational criticisms of these new methods (oddly enough, now called
``classical''). He proposed a way of having one's cake and eating it
too:
\begin{enumerate}
\item Represent classical methods with formal axioms and rules;
  represent mathematical questions as !!{formula}s in an axiomatic
  system.
\item Use safe, ``finitary'' methods to prove that these formal
  deductive systems are consistent.
\end{enumerate}

Hilbert's work went a long way toward accomplishing the first goal.
In 1899, he had done this for geometry in his celebrated book
\emph{Foundations of geometry}. In subsequent years, he and a number
of his students and collaborators worked on other areas of mathematics
to do what Hilbert had done for geometry.  Hilbert himself gave axiom
systems for arithmetic and analysis. Zermelo gave an axiomatization of
set theory, which was expanded on by Fraenkel, Skolem, von Neumann,
and others.  By the mid-1920s, there were two approaches that laid
claim to the title of an axiomatization of ``all'' of mathematics, the
\emph{Principia mathematica} of Russell and Whitehead, and what came to
be known as Zermelo-Fraenkel set theory.

In 1921, Hilbert set out on a research project to establish the goal
of proving these systems to be consistent.  He was aided in this
project by several of his students, in particular Bernays, Ackermann,
and later Gentzen. The basic idea for accomplishing this goal was to
cast the question of the possibility of a derivation of an
inconsistency in mathmatics as a combinatorial problem about possible
sequences of symbols, namely possible sequences of sentences which
meet the criterion of being a correct derivation of, say, $!A \land
\lnot !A$ from the axioms of an axiom system for arithmetic, analysis,
or set theory.  A proof of the impossibility of such a sequence of
symbols would---since it is itself a mathematical proof---be
formalizable in these axiomatic systems.  In other words, there would
be some sentence $\OCon$ which states that, say, arithmetic is
consistent.  Moreover, this sentence should be provable in the systems
in question, especially if its proof requires only very restricted,
``finitary'' means.

The second aim, that the axiom systems developed would settle every
mathematical question, can be made precise in two ways. In one way, we
can formulate it as follows: For any sentence~$!A$ in the language of
an axiom system for mathematics, either $!A$ or $\lnot !A$ is provable
from the axioms.  If this were true, then there would be no sentences
which can neither be proved nor refuted on the basis of the axioms, no
questions which the axioms do not settle.  An axiom system with this
property is called \emph{complete}. Of course, for any given sentence
it might still be a difficult task to determine which of the two
alternatives holds.  But in principle there should be a method to do
so.  In fact, for the axiom and derivation systems considered by
Hilbert, completeness would imply that such a method exists---although
Hilbert did not realize this.  The second way to interpret the
question would be this stronger requirement: that there be a
mechanical, computational method which would determine, for a given
sentence~$!A$, whether it is derivable from the axioms or not.

In 1931, G\"odel proved the two ``incompleteness theorems,'' which
showed that this program could not succeed. There is no axiom system
for mathematics which is complete, specifically, the sentence that
expresses the consistency of the axioms is a sentence which can
neither be proved nor refuted.

This struck a lethal blow to Hilbert's original program. However, as
is so often the case in mathematics, it also opened up exciting new
avenues for research. If there is no one, all-encompassing formal
system of mathematics, it makes sense to develop more circumscribesd
systems and investigate what can be proved in them. It also makes
sense to develop less restricted methods of proof for establishing the
consistency of these systems, and to find ways to measure how hard it
is to prove their consistency.  Since G\"odel showed that (almost)
every formal system has questions it cannot settle, it makes sense to
look for ``interesting'' questions a given formal system cannot
settle, and to figure out how strong a formal system has to be to
settle them. To the present day, logicians have been pursuing these
questions in a new mathematical discipline, the theory of proofs.

\end{document}
